"""
Comprehensive tests for utils/embeddings.py module
Tests all EmbeddingManager methods including multilingual support,
lazy loading, batch processing, and error handling.
"""

import pytest
import math
from unittest.mock import Mock, patch, MagicMock
from utils.embeddings import EmbeddingManager


class TestEmbeddingManagerInitialization:
    """Test EmbeddingManager initialization and model loading"""
    
    @patch('utils.embeddings.OpenAI')
    def test_default_model_initialization(self, mock_openai):
        """Test initialization with default multilingual model"""
        manager = EmbeddingManager()
        
        assert manager.model_name == "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        assert manager.model is not None
        assert manager.embedding_dimension > 0
    
    @patch('utils.embeddings.OpenAI')
    def test_custom_model_initialization(self, mock_openai):
        """Test initialization with custom model name"""
        custom_model = "sentence-transformers/all-MiniLM-L6-v2"
        manager = EmbeddingManager(model_name=custom_model)
        
        assert manager.model_name == custom_model
        assert manager.model is not None
    
    @patch('utils.embeddings.OpenAI')
    def test_lazy_loading_model_property(self, mock_openai):
        """Test that model is loaded immediately on initialization"""
        manager = EmbeddingManager()
        
        # Model should be loaded during __init__
        assert manager.model is not None
        assert hasattr(manager.model, 'encode')
    
    @patch('utils.embeddings.OpenAI')
    def test_embedding_dimension_set_correctly(self, mock_openai):
        """Test that embedding dimension is captured during initialization"""
        manager = EmbeddingManager()
        
        # Default multilingual model should have 1536 dimensions
        assert manager.embedding_dimension == 1536
        assert isinstance(manager.embedding_dimension, int)


class TestGenerateEmbeddings:
    """Test batch embeddings generation"""
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_single_text(self, mock_openai):
        """Test generating embeddings for single text"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        texts = ["This is a test document"]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (1, manager.embedding_dimension)
        assert hasattr(embeddings, 'shape')
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_multiple_texts(self, mock_openai):
        """Test generating embeddings for multiple texts"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        texts = [
            "First document about UAE law",
            "Second document about regulations",
            "Third document about legal framework"
        ]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (3, manager.embedding_dimension)
        assert hasattr(embeddings, 'shape')
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_batch_processing(self, mock_openai):
        """Test batch processing with many texts"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536) for _ in range(100)]
        )
        
        manager = EmbeddingManager()
        # Create 100 texts to test batching (default batch_size=32)
        texts = [f"Document number {i} about legal matters" for i in range(100)]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (100, manager.embedding_dimension)
        assert hasattr(embeddings, 'shape')
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_empty_list_raises_error(self, mock_openai):
        """Test that empty list raises ValueError"""
        manager = EmbeddingManager()
        
        with pytest.raises(ValueError, match="Cannot generate embeddings for empty text list"):
            manager.generate_embeddings([])
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_preserves_order(self, mock_openai):
        """Test that embeddings maintain input text order"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        texts = ["Text A", "Text B", "Text C"]
        
        embeddings1 = manager.generate_embeddings(texts)
        embeddings2 = manager.generate_embeddings(texts)
        
        # Same texts should produce same embeddings in same order
        assert embeddings1.shape == embeddings2.shape
        for i in range(len(texts)):
            for j in range(manager.embedding_dimension):
                assert abs(embeddings1[i][j] - embeddings2[i][j]) < 1e-5
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_embeddings_calls_encode_correctly(self, mock_openai):
        """Test that encode is called with correct parameters"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        texts = ["Text 1", "Text 2"]
        
        embeddings = manager.generate_embeddings(texts)
        
        # Verify shape matches expected output
        assert embeddings.shape == (2, 1536)


class TestGenerateQueryEmbedding:
    """Test single query embedding generation"""
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_query_embedding_valid_query(self, mock_openai):
        """Test generating embedding for valid query"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        query = "What are the labor laws in UAE?"
        
        embedding = manager.generate_query_embedding(query)
        
        assert embedding.shape == (manager.embedding_dimension,)
        assert hasattr(embedding, 'shape')
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_query_embedding_empty_string_raises_error(self, mock_openai):
        """Test that empty query raises ValueError"""
        manager = EmbeddingManager()
        
        with pytest.raises(ValueError, match="Cannot generate embedding for empty query"):
            manager.generate_query_embedding("")
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_query_embedding_whitespace_only_raises_error(self, mock_openai):
        """Test that whitespace-only query raises ValueError"""
        manager = EmbeddingManager()
        
        with pytest.raises(ValueError, match="Cannot generate embedding for empty query"):
            manager.generate_query_embedding("   \t\n  ")
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_query_embedding_consistency(self, mock_openai):
        """Test that same query produces same embedding"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        query = "Test query for consistency"
        
        embedding1 = manager.generate_query_embedding(query)
        embedding2 = manager.generate_query_embedding(query)
        
        assert len(embedding1) == len(embedding2)
        for i in range(len(embedding1)):
            assert abs(embedding1[i] - embedding2[i]) < 1e-5
    
    @patch('utils.embeddings.OpenAI')
    def test_generate_query_embedding_no_progress_bar(self, mock_openai):
        """Test that query embedding doesn't show progress bar"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        
        embedding = manager.generate_query_embedding("test query")
        
        # Should produce valid embedding
        assert embedding.shape == (1536,)


class TestMultilingualSupport:
    """Test multilingual embedding support (Arabic, English, Slovak)"""
    
    @patch('utils.embeddings.OpenAI')
    def test_english_text_embedding(self, mock_openai):
        """Test embedding generation for English text"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        texts = ["This is an English legal document about UAE regulations"]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (1, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])
    
    @patch('utils.embeddings.OpenAI')
    def test_arabic_text_embedding(self, mock_openai):
        """Test embedding generation for Arabic text"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        texts = ["Ù‡Ø°Ø§ Ù…Ø³ØªÙ†Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¹Ù† Ø§Ù„ØªØ´Ø±ÙŠØ¹Ø§Øª Ø§Ù„Ø¥Ù…Ø§Ø±Ø§ØªÙŠØ©"]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (1, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])
    
    @patch('utils.embeddings.OpenAI')
    def test_slovak_text_embedding(self, mock_openai):
        """Test embedding generation for Slovak text"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        texts = ["Toto je prÃ¡vny dokument o prÃ¡vnych predpisoch SAE"]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (1, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])
    
    @patch('utils.embeddings.OpenAI')
    def test_mixed_language_batch(self, mock_openai):
        """Test embedding generation for mixed language batch"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        texts = [
            "English legal text about regulations",
            "Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù† Ø§Ù„Ø£Ù†Ø¸Ù…Ø©",
            "SlovenskÃ½ prÃ¡vny text o predpisoch"
        ]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (3, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])
        
        # Verify all embeddings are different (not zeros or same values)
        assert not all(abs(embeddings[0][i] - embeddings[1][i]) < 1e-6 for i in range(manager.embedding_dimension))
        assert not all(abs(embeddings[1][i] - embeddings[2][i]) < 1e-6 for i in range(manager.embedding_dimension))
    
    @patch('utils.embeddings.OpenAI')
    def test_arabic_query_embedding(self, mock_openai):
        """Test query embedding for Arabic text"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        query = "Ù…Ø§ Ù‡ÙŠ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§ØªØŸ"
        
        embedding = manager.generate_query_embedding(query)
        
        assert embedding.shape == (manager.embedding_dimension,)
        for i in range(len(embedding)):
            assert not math.isnan(embedding[i])
    
    @patch('utils.embeddings.OpenAI')
    def test_multilingual_semantic_similarity(self, mock_openai):
        """Test that semantically similar texts in different languages have similar embeddings"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        
        # Similar legal concept in different languages
        english_text = "labor law regulations"
        arabic_text = "Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„"
        
        emb_en = manager.generate_query_embedding(english_text)
        emb_ar = manager.generate_query_embedding(arabic_text)
        
        # Calculate cosine similarity
        dot_product = sum(emb_en[i] * emb_ar[i] for i in range(len(emb_en)))
        norm_en = math.sqrt(sum(x * x for x in emb_en))
        norm_ar = math.sqrt(sum(x * x for x in emb_ar))
        similarity = dot_product / (norm_en * norm_ar)
        
        # Should have some similarity (> 0.3) as they're related concepts
        # Note: This is a loose test as translation isn't perfect
        assert similarity > 0.2  # Relaxed threshold for multilingual


class TestGetEmbeddingDimension:
    """Test embedding dimension getter"""
    
    @patch('utils.embeddings.OpenAI')
    def test_get_embedding_dimension_returns_correct_value(self, mock_openai):
        """Test that dimension getter returns correct value"""
        manager = EmbeddingManager()
        
        dim1 = manager.get_embedding_dimension()
        dim2 = manager.embedding_dimension
        
        assert dim1 == dim2
        assert isinstance(dim1, int)
        assert dim1 > 0
    
    @patch('utils.embeddings.OpenAI')
    def test_get_embedding_dimension_consistency(self, mock_openai):
        """Test that dimension remains consistent across calls"""
        manager = EmbeddingManager()
        
        dim1 = manager.get_embedding_dimension()
        dim2 = manager.get_embedding_dimension()
        
        assert dim1 == dim2


class TestGetModelInfo:
    """Test model information retrieval"""
    
    @patch('utils.embeddings.OpenAI')
    def test_get_model_info_returns_dict(self, mock_openai):
        """Test that model info returns dictionary"""
        manager = EmbeddingManager()
        
        info = manager.get_model_info()
        
        assert isinstance(info, dict)
    
    @patch('utils.embeddings.OpenAI')
    def test_get_model_info_contains_required_fields(self, mock_openai):
        """Test that model info contains all required fields"""
        manager = EmbeddingManager()
        
        info = manager.get_model_info()
        
        assert "model_name" in info
        assert "embedding_dimension" in info
        assert "max_sequence_length" in info
        assert "supports_languages" in info
    
    @patch('utils.embeddings.OpenAI')
    def test_get_model_info_correct_values(self, mock_openai):
        """Test that model info values are correct"""
        manager = EmbeddingManager()
        
        info = manager.get_model_info()
        
        assert info["model_name"] == manager.model_name
        assert info["embedding_dimension"] == manager.embedding_dimension
        assert isinstance(info["max_sequence_length"], int)
        assert isinstance(info["supports_languages"], list)
    
    @patch('utils.embeddings.OpenAI')
    def test_get_model_info_language_support(self, mock_openai):
        """Test that supported languages are listed"""
        manager = EmbeddingManager()
        
        info = manager.get_model_info()
        
        languages = info["supports_languages"]
        assert "Arabic" in languages
        assert "English" in languages
        assert "Slovak" in languages
        assert "50+ others" in languages


class TestErrorHandling:
    """Test error handling and edge cases"""
    
    @patch('utils.embeddings.OpenAI')
    def test_none_text_in_batch_handling(self, mock_openai):
        """Test handling of None values in text batch"""
        manager = EmbeddingManager()
        
        # Most implementations will convert None to string or error
        # Test the actual behavior
        texts = ["Valid text", None, "Another text"]
        
        # Should either handle gracefully or raise TypeError
        try:
            embeddings = manager.generate_embeddings(texts)
            # If it succeeds, verify shape
            assert embeddings.shape[0] == 3
        except (TypeError, AttributeError):
            # This is also acceptable behavior
            pass
    
    @patch('utils.embeddings.OpenAI')
    def test_very_long_text_handling(self, mock_openai):
        """Test handling of very long text (exceeds max_seq_length)"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        
        # Create text longer than typical max_seq_length (512 tokens)
        long_text = "word " * 1000
        
        # Should truncate and still produce embedding
        embedding = manager.generate_query_embedding(long_text)
        
        assert embedding.shape == (manager.embedding_dimension,)
        for i in range(len(embedding)):
            assert not math.isnan(embedding[i])
    
    @patch('utils.embeddings.OpenAI')
    def test_special_characters_handling(self, mock_openai):
        """Test handling of special characters and symbols"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        
        texts = [
            "Text with symbols: @#$%^&*()",
            "Text with numbers: 123456789",
            "Text with punctuation: ... !!! ???"
        ]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (3, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])
    
    @patch('utils.embeddings.OpenAI')
    def test_unicode_handling(self, mock_openai):
        """Test handling of various unicode characters"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536),
                MagicMock(embedding=[0.4] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        
        texts = [
            "Emoji test: ðŸ˜€ ðŸ“š âš–ï¸",
            "Arabic: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
            "Chinese: ä¸­æ–‡",
            "Mixed: Test Ù…Ø®ØªÙ„Ø· mixed"
        ]
        
        embeddings = manager.generate_embeddings(texts)
        
        assert embeddings.shape == (4, manager.embedding_dimension)
        for i in range(embeddings.shape[0]):
            for j in range(embeddings.shape[1]):
                assert not math.isnan(embeddings[i][j])


class TestEmbeddingQuality:
    """Test embedding quality and properties"""
    
    @patch('utils.embeddings.OpenAI')
    def test_embeddings_are_normalized_or_reasonable_range(self, mock_openai):
        """Test that embedding values are in reasonable range"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        texts = ["Sample legal document text"]
        
        embeddings = manager.generate_embeddings(texts)
        
        # Values should typically be in range [-1, 1] or [-2, 2]
        min_val = min(embeddings[i][j] for i in range(embeddings.shape[0]) for j in range(embeddings.shape[1]))
        max_val = max(embeddings[i][j] for i in range(embeddings.shape[0]) for j in range(embeddings.shape[1]))
        assert min_val >= -5.0
        assert max_val <= 5.0
    
    @patch('utils.embeddings.OpenAI')
    def test_different_texts_produce_different_embeddings(self, mock_openai):
        """Test that different texts produce different embeddings"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        
        def create_response(*args, **kwargs):
            return MagicMock(data=[MagicMock(embedding=[0.1 + i * 0.1 for i in range(1536)])])
        
        mock_client.embeddings.create.side_effect = create_response
        
        manager = EmbeddingManager()
        
        text1 = "UAE labor law regulations"
        text2 = "Corporate governance framework"
        
        emb1 = manager.generate_query_embedding(text1)
        emb2 = manager.generate_query_embedding(text2)
        
        # Embeddings should be different
        assert not all(abs(emb1[i] - emb2[i]) < 1e-6 for i in range(len(emb1)))
    
    @patch('utils.embeddings.OpenAI')
    def test_similar_texts_produce_similar_embeddings(self, mock_openai):
        """Test that similar texts produce similar embeddings"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536)]
        )
        
        manager = EmbeddingManager()
        
        text1 = "labor law regulations in UAE"
        text2 = "UAE employment law and regulations"
        
        emb1 = manager.generate_query_embedding(text1)
        emb2 = manager.generate_query_embedding(text2)
        
        # Calculate cosine similarity
        dot_product = sum(emb1[i] * emb2[i] for i in range(len(emb1)))
        norm1 = math.sqrt(sum(x * x for x in emb1))
        norm2 = math.sqrt(sum(x * x for x in emb2))
        similarity = dot_product / (norm1 * norm2)
        
        # Similar texts should have high similarity (> 0.5)
        assert similarity > 0.5


class TestBatchProcessingEfficiency:
    """Test batch processing behavior"""
    
    @patch('utils.embeddings.OpenAI')
    def test_batch_processing_consistency(self, mock_openai):
        """Test that batch and individual processing give same results"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
                MagicMock(embedding=[0.3] * 1536)
            ]
        )
        
        manager = EmbeddingManager()
        texts = ["Text A", "Text B", "Text C"]
        
        # Batch processing
        batch_embeddings = manager.generate_embeddings(texts)
        
        # Individual processing
        individual_embeddings = [
            manager.generate_query_embedding(text) for text in texts
        ]
        
        # Should produce same results (within floating point tolerance)
        for i in range(len(texts)):
            for j in range(manager.embedding_dimension):
                assert abs(batch_embeddings[i][j] - individual_embeddings[i][j]) < 1e-4
    
    @patch('utils.embeddings.OpenAI')
    def test_batch_size_parameter_used(self, mock_openai):
        """Test that batch_size parameter affects processing"""
        mock_client = MagicMock()
        mock_openai.return_value = mock_client
        mock_client.embeddings.create.return_value = MagicMock(
            data=[MagicMock(embedding=[0.1] * 1536) for _ in range(5)]
        )
        
        manager = EmbeddingManager()
        texts = ["Text"] * 5
        
        embeddings = manager.generate_embeddings(texts)
        
        # Verify correct shape
        assert embeddings.shape == (5, 1536)


# Fixtures for common test data
@pytest.fixture
def sample_texts():
    """Fixture providing sample texts for testing"""
    return [
        "UAE Federal Law on Commercial Companies",
        "Employment regulations in Dubai",
        "Intellectual property rights in UAE"
    ]


@pytest.fixture
def sample_arabic_texts():
    """Fixture providing sample Arabic texts"""
    return [
        "Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø§ØªØ­Ø§Ø¯ÙŠ Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©",
        "Ù„ÙˆØ§Ø¦Ø­ Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙÙŠ Ø¯Ø¨ÙŠ",
        "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ù„ÙƒÙŠØ© Ø§Ù„ÙÙƒØ±ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"
    ]


@pytest.fixture
def embedding_manager():
    """Fixture providing initialized EmbeddingManager"""
    with patch('utils.embeddings.OpenAI'):
        return EmbeddingManager()